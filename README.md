# atmasim
If you are here, you are probably familiar with Magicsim, which builds heavily on the notion of composite sims. Scaling SimC naively is a fairly straightforward task to engineer using technologies like CloudFormation and load balancing. Magicsim profiles however present a unique challenge in that results must be collated and averaged by weights and emitted.

Atmasim solves this by using MapReduce to massively parallelize SimulationCraft for usage on Hadoop-backed clusters. It takes a CSV format key-value pair list of simulations to run and how to group them and internally it maps sim profiles to DPS using SimC and reduces DPS by merging them using a magicsim profile. It outputs a directory structure that collects data by input parameters and has a list of files, each corresponding to a model with the parameters given by its directory structure.

It's optimized to work using Amazon's ElasticMapReduce and shows significant scalability. Using roughly 576 cores and 2160 TB of RAM I was able to process roughly peak 115,000 simulations per hour or roughly 2 per VM per second (c4.8xlarge) using a Java implementation with Hadoop. Using further optimizations on SimC compilation, 5%+ performance could be obtained. Using a thinner Scala Spark implementation may further improve the margin of performance.
